{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************************\n",
    "#                          CLASICAL\n",
    "#                          \n",
    "#***************************************************************************\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "\n",
    "class C_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim=128,  n_filters=16, output_dim=128, dropout=0.2,  n_output=1, dilaSize=1):\n",
    "        \n",
    "        \n",
    "        super(C_Net, self).__init__()\n",
    "\n",
    "        self.embed_smile = nn.Embedding(65, embed_dim)\n",
    "        self.embed_prot = nn.Embedding(26, embed_dim)\n",
    "        \n",
    "        #smiles \n",
    "        self.smiles = nn.Sequential(\n",
    "            nn.Conv1d(in_channels= embed_dim, out_channels= embed_dim, kernel_size=3,padding=dilaSize, dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 2 ,dilation=dilaSize * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            nn.ReLU(),\n",
    "           nn.AdaptiveMaxPool1d(1)                           \n",
    "        )\n",
    "            \n",
    "        #proteins sequence\n",
    "        self.proteins = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=3,padding=dilaSize ,dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim  , out_channels=embed_dim, kernel_size=3,padding=dilaSize *2 ,dilation=dilaSize *2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "          )\n",
    "        \n",
    "\n",
    "        #self.smiles_descriptors =  nn.Sequential(    #With descriptors\n",
    "        #     nn.Linear(6 , 6),\n",
    "        #     nn.ReLU()\n",
    "         #)\n",
    "            \n",
    "        #self.linear = nn.Linear(embed_dim,  output_dim)\n",
    "        \n",
    "        self.layer2 = 128;\n",
    "        self.layer3 =  64;\n",
    "    \n",
    "        \n",
    "\n",
    "        self.predict = nn.Sequential(\n",
    "                                    #nn.Linear(2 * embed_dim + 6, 1024),  #With descriptors\n",
    "                                    nn.Linear(2 * output_dim, self.layer2 ),   #no descriptors\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(dropout),\n",
    "                                    nn.Linear(self.layer2 , self.layer3),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(dropout),\n",
    "                                    nn.Linear(self.layer3 , n_output)#,\n",
    "                                    #nn.Sigmoid()           #7/4/24          #sigmoid giati to target exei times 0 -1, diaforetika den bazw tipota\n",
    "                                    )\n",
    "  \n",
    "    def forward(self, smi_in, seq_in,  smi_desc):#65,26\n",
    "        \n",
    "        embedded_smi  = self.embed_smile(smi_in) \n",
    "        embedded_seq  = self.embed_prot(seq_in)\n",
    "\n",
    "      \n",
    "        smi = self.smiles(embedded_smi.transpose(1,2))\n",
    "        seq = self.proteins(embedded_seq.transpose(1,2))\n",
    "        \n",
    "\n",
    "        smi = smi.squeeze()\n",
    "        seq = seq.squeeze()\n",
    "        \n",
    "        # concat\n",
    "        #smi_seq = torch.cat((smi, seq, smi_desc),1)   #With descriptors\n",
    "\n",
    "        smi_seq = torch.cat((smi, seq),1)           #no descriptors\n",
    "        \n",
    "        out = self.predict(smi_seq)\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************************\n",
    "#                          HYBRID\n",
    "#\n",
    "#******************************************************************\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "import math\n",
    "import pennylane as qml\n",
    "from functools import partial\n",
    "import pennylane.numpy as np\n",
    "\n",
    "\n",
    "n_qubits  = 4\n",
    "n_layers  = 3 \n",
    "n_features = 256\n",
    "n_angle_encoding = math.ceil(((n_features)/n_qubits)/2) #dense angle\n",
    "#n_angle_encoding = math.ceil(((n_features)/n_qubits)) #angle\n",
    "batch_size = 256\n",
    "n_blocks = 2\n",
    "\n",
    "# default.qubit RUN ALL EXPERIMENTS**********\n",
    "#dev = qml.device(\"default.qubit\", wires=n_qubits )\n",
    "#dev = qml.device(\"lightning.qubit\", wires=n_qubits )  # NOT USED*************\n",
    "\n",
    "#  NOISE SIMULATORS************************************8\n",
    "# Describe noise\n",
    "#noise_gate = qml.AmplitudeDamping\n",
    "#noise_gate = qml.DepolarizingChannel\n",
    "#noise_strength = 0.1\n",
    "\n",
    "# Load devices\n",
    "dev_ideal = qml.device(\"default.mixed\", wires=n_qubits)  # the ideal device with no noise\n",
    "#dev_noisy = qml.transforms.insert(noise_gate, noise_strength)(dev_ideal) # the noisy device with noise\n",
    "#dev_noisy = qml.transforms.insert(dev_ideal, noise_gate, noise_strength, position=\"all\")\n",
    "\n",
    "\n",
    "\n",
    "def circuit_amplitude(inputs, weights):                 #inputs  256 (batch size) x 256  output previous layer\n",
    "\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True )\n",
    "\n",
    "    for W in weights: \n",
    "        for i in range(n_qubits):\n",
    "            qml.Rot(*W[i], wires=i)                                   \n",
    "\n",
    "        for i in range(n_qubits):\n",
    "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "\n",
    "    for i in range(n_qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % n_qubits, 0])\n",
    "         \n",
    "    return  qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "\n",
    "# einai isodinamo me to panw mono poy to cnot = range[1,2,3] ενώ το πάνω είναι range[1,1,1]\n",
    "def circuit_angle_qml(inputs, weights):\n",
    "    for i in range(n_angle_encoding):\n",
    "        qml.AngleEmbedding(math.pi * inputs[:,i*n_qubits:i*n_qubits+n_qubits], wires=range(n_qubits))    #rotation='X'\n",
    "        qml.StronglyEntanglingLayers(weights[i], wires = range (n_qubits), ranges=[1,1,1])   \n",
    "\n",
    "    # prosthesa CNOT  me 2 orisma to qubit 1 22/4/2024\n",
    "    for i in range(n_qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % n_qubits, 0])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "# ta dedomena apo ta klasika layer prepei na  ginoun normalize\n",
    "#@qml.qnode(dev)\n",
    "def circuit_dense_angle(inputs, weights):                 #inputs  256 (batch size) x 256  output previous layer\n",
    "\n",
    "    for j in range(n_angle_encoding):               \n",
    "        for i in range(n_qubits):                               #encoding\n",
    "            qml.RX(math.pi *  inputs[:, i*2   + n_qubits *j] ,  wires=i)      #dense endocoding kai oxi aplo\n",
    "            qml.RY(math.pi *  inputs[:, i*2+1 + n_qubits *j],   wires=i)\n",
    "\n",
    "        qml.StronglyEntanglingLayers(weights[j], wires = range ( n_qubits ), ranges=[1,1,1])  \n",
    "\n",
    "    for i in range(n_qubits-1):\n",
    "         qml.CNOT(wires=[ (i + 1) % n_qubits, 0])\n",
    "            \n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "\n",
    "class HQ_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim=128,  n_filters=16, output_dim=128, dropout=0.2,  n_output=1, dilaSize=1, normalize=True):\n",
    "        \n",
    "        \n",
    "        super(HQ_Net, self).__init__()\n",
    "\n",
    "        self.embed_smile = nn.Embedding(65, embed_dim)\n",
    "        self.embed_prot = nn.Embedding(26, embed_dim)\n",
    "        \n",
    "        #smiles \n",
    "        self.smiles = nn.Sequential(\n",
    "            nn.Conv1d(in_channels= embed_dim, out_channels= embed_dim, kernel_size=3,padding=dilaSize, dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 2 ,dilation=dilaSize * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            #nn.ReLU(), 7/4/2024 return 0 - 1\n",
    "            nn.Sigmoid(), \n",
    "            nn.AdaptiveMaxPool1d(1)                        \n",
    "        )\n",
    "            \n",
    "        #proteins sequence\n",
    "        self.proteins = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=3,padding=dilaSize ,dilation=dilaSize),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim  , out_channels=embed_dim, kernel_size=3,padding=dilaSize *2 ,dilation=dilaSize *2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 4 ,dilation=dilaSize * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 8 ,dilation=dilaSize * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= embed_dim , out_channels= embed_dim, kernel_size=3,padding=dilaSize * 16,dilation=dilaSize * 16),\n",
    "            #nn.ReLU(), 7/4/2024 return 0 - 1\n",
    "            nn.Sigmoid(),   \n",
    "            nn.AdaptiveMaxPool1d(1),\n",
    "          )\n",
    "        \n",
    "\n",
    "        #self.smiles_descriptors =  nn.Sequential(\n",
    "        #     nn.Linear(6 , 6),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "            \n",
    "        #self.predict = nn.Sequential(\n",
    "        #                            #nn.Linear(2 * embed_dim + 6, 1024), #6 smiles descriptors                                    #nn.Linear(2 * embed_dim + 6, 1024), #6 smiles descriptors\n",
    "        #                            nn.Linear(2 * embed_dim , 1024), #6 smiles descriptors\n",
    "        #                            nn.ReLU(),\n",
    "        #                            nn.Dropout(dropout),\n",
    "        #                            nn.Linear(1024, 512),\n",
    "        #                            nn.ReLU(),\n",
    "         #                           nn.Dropout(dropout),\n",
    "         #                           nn.Linear(512, n_output)\n",
    "         #                           )\n",
    "      \n",
    "        #for amplitube\n",
    "        #weight_shapes = {\"weights\": ( n_layers, n_qubits, 3)}\n",
    "\n",
    "        # for angle\n",
    "        weight_shapes = {\"weights\": (n_angle_encoding, n_layers, n_qubits, 3)}\n",
    "\n",
    "       \n",
    "        #qnode = qml.QNode(circuit_amplitude_qml, dev, interface='torch', diff_method='backprop')\n",
    "        #qnode = qml.QNode(circuit_angle_qml, dev, interface='torch', diff_method='backprop')\n",
    "        qnode = qml.QNode(circuit_dense_angle, dev_ideal, interface='torch', diff_method='backprop')\n",
    "        \n",
    "        self.predict_q = qml.qnn.TorchLayer(qnode, weight_shapes) \n",
    "        \n",
    "  \n",
    "    def forward(self, smi_in, seq_in,  smi_desc):#65,26\n",
    "    \n",
    "        embedded_smi  = self.embed_smile(smi_in) \n",
    "        embedded_seq  = self.embed_prot(seq_in)\n",
    "                \n",
    "        smi = self.smiles(embedded_smi.transpose(1,2))\n",
    "        seq = self.proteins(embedded_seq.transpose(1,2))\n",
    "        \n",
    "        smi = smi.squeeze()\n",
    "        seq = seq.squeeze()\n",
    "\n",
    "        # concat\n",
    "        #smi_seq = torch.cat((smi, seq, smi_desc),1)\n",
    "        #print('smi.shape', smi.shape)\n",
    "        #print('smi', smi )\n",
    "        smi_seq  =torch.cat((smi, seq) , 1)  \n",
    "        \n",
    "        out = self.predict_q(smi_seq)            \n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "\n",
    "def test(model: nn.Module, test_loader, loss_function, device, show, _p, record):\n",
    "    path = '../run/'\n",
    "    #path = \"C:/Experiments/HQ-DTA/run/\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (*x, y) in tqdm(enumerate(test_loader), disable=not show, total=len(test_loader)):\n",
    "            for i in range(len(x)):\n",
    "                x[i] = x[i].to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = model(*x)\n",
    "\n",
    "            test_loss += loss_function(y_hat.view(-1), y.view(-1)).item()\n",
    "            outputs.append(y_hat.cpu().numpy().reshape(-1))\n",
    "            targets.append(y.cpu().numpy().reshape(-1))\n",
    "\n",
    "    targets = np.concatenate(targets).reshape(-1)\n",
    "    outputs = np.concatenate(outputs).reshape(-1)\n",
    "\n",
    "    np.savetxt(path + _p + record + 'targets.csv', targets, fmt='%1.2f' )#, fmt ='%d'\n",
    "    np.savetxt(path + _p + record + 'outputs.csv', outputs, fmt='%1.2f')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    evaluation = {\n",
    "        'loss': test_loss, #einai idio me to MSE\n",
    "        'MSE':  metrics.MSE(targets, outputs),\n",
    "        'RMSE': metrics.RMSE(targets, outputs),\n",
    "        'R2':   metrics.R2(targets, outputs),\n",
    "        #'R2 adjusted':   R2_adjusted(targets, outputs),  # y_train, X_train\n",
    "        'MAE': metrics.MAE(targets, outputs),\n",
    "        'Person': metrics.PERSON(targets, outputs),\n",
    "        'p_value': metrics.P_VALUE(targets, outputs),\n",
    "        'C_INDEX': metrics.C_INDEX(targets, outputs),\n",
    "        'SD': metrics.SD(targets, outputs),\n",
    "    }\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
     "The training code was based on the code of Kaili Wang, 2021 https://github.com/KailiWang1/DeepDTAF\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "#import numpy as np\n",
    "import pennylane.numpy as np\n",
    "import torch\n",
    "from torch import _pin_memory, nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from dataset import MyDataset_PDBBind2020, get_scalers_PDBBind2020, MyDataset_pdbbind2016, MyDataset_davis_kiba, get_scalers_davis_kiba\n",
    "\n",
    "\n",
    "print(sys.argv)\n",
    "\n",
    "SHOW_PROCESS_BAR = True\n",
    "\n",
    "\n",
    "seed = np.random.randint(33927, 33928) ##random \n",
    "path = Path(f'../export/{datetime.now().strftime(\"%m%d%H%M\")}_{seed}') \n",
    "\n",
    "output_name= datetime.now().strftime(\"%m%d%H%M\");\n",
    "\n",
    "cuda_name = \"cuda:0\"\n",
    "device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_seq_len = 1000  \n",
    "max_smi_len = 160\n",
    "\n",
    "batch_size = 256\n",
    "n_epoch = 30  \n",
    "interrupt = None\n",
    "save_best_epoch = 5  # init 5  when `save_best_epoch` is reached and the loss starts to decrease, save best model parameters\n",
    "scale_target = True # For Hybrid montel the target must be scaled \n",
    "scale_inputs = False  \n",
    "init_weights = False\n",
    "\n",

    "\n",
    "# GPU uses cudnn as backend to ensure repeatable by setting the following (in turn, use advances function to speed up training)\n",
    "torch.backends.cudnn.deterministic = False \n",
    "torch.backends.cudnn.benchmark =  True\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print('path: ', path)\n",
    "\n",
    "writer = SummaryWriter(path)\n",
    "f_param = open(path / 'parameters.txt', 'w')\n",
    "\n",
    "print(f'device = {device}')\n",
    "print(f'seed = {seed}')\n",
    "print(f'write to {path}')\n",
    "print(f'batch_size={batch_size}')\n",
    "print(f'epoch = {n_epoch}')\n",
    "print(f'Scale target = {scale_target}')\n",
    "print(f'Scale inputs = {scale_inputs}')\n",
    "print(f'init_weights = {init_weights}')\n",
    "\n",
    "\n",
    "\n",
    "f_param.write(f'device = {device}\\n'\n",
    "          f'seed = {seed}\\n'\n",
    "          f'write to {path}\\n'\n",
    "          f'batch_size= {batch_size}\\n'\n",
    "          f'epoch = {n_epoch}\\n'\n",
    "          f'Scale target= {scale_target}\\n'  \n",
    "          f'Scale inputs= {scale_inputs}\\n'\n",
    "          f'init_weights = {init_weights}') \n",
    "\n",
    "\n",
    "\n",
    "#******************* NOT USED**************\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        #nn.init.uniform_(m.weight.data, a=0.0, b=0.01)\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        #nn.init.uniform_(m.weight.data, a=0.0, b=0.01)\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "#********************************************      \n",
    "\n",
    "assert 0<=save_best_epoch<n_epoch, 'Save_best_epoch parameter must be greater than n_epoch '\n",
    "\n",
    "\n",
    "modeling = [C_Net, HQ_Net][1]\n",
    "\n",
    "data_name = ['pdbbind2020', 'pdbbind2016', 'davis','kiba'][1]\n",
    "\n",
    "\n",
    "\n",
    "f_param.write(f'data base={data_name}\\n')\n",
    "\n",
    "data_path = '../data/' + data_name\n",
    "print(f'data path to {data_path}')\n",
    "\n",
    "f_param.write(f'data_path={data_path}\\n')\n",
    "\n",
    "model = modeling().to(device)\n",
    "#aply init weights\n",
    "if init_weights:\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "print(model)\n",
    "\n",
    "f_param.write('model: \\n')\n",
    "f_param.write(str(model)+'\\n')\n",
    "#f_param.close()\n",
    "\n",
    "\n",
    "scalers=list()  \n",
    "if data_name == 'pdbbind2020':\n",
    "    scalers = get_scalers_PDBBind2020(data_path, 'training', data_name)\n",
    "    phase_name_array = ['training', 'validation', 'test']\n",
    "    max_smi_len = 160\n",
    "\n",
    "    data_loaders = {phase_name:\n",
    "                    DataLoader(MyDataset_PDBBind2020(data_path, phase_name, data_name, max_seq_len, max_smi_len, scale_target,\n",
    "                                           scale_inputs, scalers),\n",
    "                                         batch_size=batch_size,\n",
    "                                         pin_memory=True,\n",
    "                                         num_workers=4,\n",
    "                                         shuffle= True)\n",
    "                   for phase_name in phase_name_array}\n",
    "    \n",
    "elif data_name in ['davis', 'kibas']:\n",
    "    scalers = get_scalers_davis_kiba(data_path, 'training', data_name)\n",
    "    phase_name_array = ['training', 'validation', 'test']\n",
    "    if data_name == 'davis':\n",
    "        max_smi_len = 85\n",
    "    else:\n",
    "        max_smi_len = 100\n",
    "\n",
    "    data_loaders = {phase_name:\n",
    "            DataLoader(MyDataset_davis_kiba(data_path, phase_name, data_name, max_seq_len, max_smi_len, scale_target,\n",
    "                                    scale_inputs, scalers),\n",
    "                                    batch_size=batch_size,\n",
    "                                    pin_memory=True,\n",
    "                                    num_workers=4,\n",
    "                                    shuffle= True)\n",
    "                   for phase_name in phase_name_array}\n",
    "elif data_name == 'pdbbind2016': \n",
    "    print('Scaller in dataset') #scalers = get_scalers(data_path, 'training', data_name)\n",
    "    phase_name_array = ['training', 'validation', 'test', 'test105', 'test71']\n",
    "    max_smi_len = 160\n",
    "\n",
    "    data_loaders = {phase_name:\n",
    "                    DataLoader(MyDataset_pdbbind2016(data_path, phase_name, data_name, max_seq_len, max_smi_len, scale_target,\n",
    "                                           scale_inputs, scalers),\n",
    "                                         batch_size=batch_size,\n",
    "                                         pin_memory=True,\n",
    "                                         num_workers=4,\n",
    "                                         shuffle= True)\n",
    "                   for phase_name in phase_name_array}\n",
    "else:\n",
    "    print('No dataset')\n",
    "    \n",
    " \n",
    "print(f'max_seq_len={max_seq_len}\\n'\n",
    "      f'max_smi_len={max_smi_len}')\n",
    "\n",
    "f_param.write(f'max_seq_len={max_seq_len}\\n'\n",
    "      f'max_smi_len={max_smi_len}\\n')\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "optimizer = optim.AdamW(model.parameters()  )\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0001,\n",
    "                                          epochs=n_epoch,\n",
    "                                          steps_per_epoch=len(data_loaders['training']))\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    \n",
    "start = datetime.now()\n",
    "print('start at ', start)\n",
    "\n",
    "\n",
    "best_epoch = -1\n",
    "best_val_loss = 100000000\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    tbar = tqdm(enumerate(data_loaders['training']), disable= not SHOW_PROCESS_BAR, total=len(data_loaders['training']))\n",
    "    \n",
    "    #print('after tqdm, how much time takes ', datetime.now())\n",
    "    \n",
    "    for idx, (*x, y) in tbar:\n",
    "        model.train()\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i].to(device)\n",
    "\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(*x)\n",
    "        \n",
    "        loss = loss_function(output.view(-1), y.view(-1))  \n",
    "\n",
    "        loss.backward() \n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "\n",
    "        tbar.set_description(f' * Train Epoch {epoch} Loss={loss.item() / len(y):.3f}')\n",
    "\n",
    " \n",
    "    for _p in ['training', 'validation']:\n",
    "    #for _p in ['test']:\n",
    "        performance = test(model, data_loaders[_p], loss_function, device, not SHOW_PROCESS_BAR, _p, record = '_train' + str(epoch) +'_' + output_name) \n",
    "\n",
    "        for i in performance:\n",
    "            writer.add_scalar(f'{i} {_p}', performance[i], global_step=epoch)\n",
    "        if _p=='validation' and epoch>=save_best_epoch and performance['loss']<best_val_loss:\n",
    "            best_val_loss = performance['loss']\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'h_best_model.pt')\n",
    "\n",
    "print('best epoch:', best_epoch)\n",
    "f_param.write(f'best epoch={best_epoch}\\n')\n",
    "\n",
    "print('Testing...')\n",
    "\n",
    "model.load_state_dict(torch.load('h_best_model.pt'))\n",
    "with open(path / 'result.txt', 'w') as f:\n",
    "\n",
    "    for _p in phase_name_array:\n",
    "    #for _p in ['test']:\n",
    "        performance = test(model, data_loaders[_p], loss_function, device,  not SHOW_PROCESS_BAR, _p, record='_test_'+ output_name)\n",
    "        f.write(f'{_p}:\\n')\n",
    "        print(f'{_p}:')\n",
    "        for k, v in performance.items():\n",
    "            f.write(f' {k}: {v}')\n",
    "            print(f' {k}: {v}')\n",
    "        f.write('\\n')\n",
    "        print()\n",
    "\n",
    "writer.close()\n",
    "print('training finished')\n",
    "\n",
    "end = datetime.now()\n",
    "print('end at:', end)\n",
    "print('time used:', str(end - start))\n",
    "\n",
    "f_param.write(f'time used={str(end - start)}\\n')\n",
    "f_param.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
